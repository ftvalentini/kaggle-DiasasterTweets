{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-bert.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3f48cadlXxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "# todo esto de bert usa librerias tensorflow.XXX , y es todo re sensible en compatibilidades"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEiPj_KxDMuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YPziU7Hj085",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ## run just once: ### \n",
        "# !unzip data.zip\n",
        "# !unzip -o neural.zip\n",
        "# # Tokenizador de Google para Bert\n",
        "# !wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
        "# !pip install sentencepiece"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YXAMUaClnaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load BERT from the Tensorflow Hub\n",
        "import tensorflow_hub as hub\n",
        "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
        "bert_layer = hub.KerasLayer(module_url, trainable=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDJtkp5flqj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from neural.core.helpers_bert import bert_encode\n",
        "from neural.core.helpers_bert import submission_bert\n",
        "\n",
        "from neural.bert_architecture import build_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOAdkLAAlr_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RUN_SANITY = False\n",
        "NEW_RANDOM_SPLIT = True\n",
        "LOAD_PRETRAINED_VECS = False # False no genera el dict de embeddings (xq tarda unos min)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur01MNwTltHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if NEW_RANDOM_SPLIT:\n",
        "  semilla = np.random.randint(1,9999)\n",
        "else:\n",
        "  semilla = 800\n",
        "rng = np.random.RandomState(semilla)\n",
        "print(semilla)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjcfHOhOluc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import helpers_models as hm\n",
        "datos_raw = hm.read_tagged_data()\n",
        "datos = hm.clean_tagged_data(datos_raw)\n",
        "X = datos['text']\n",
        "y = datos['target']\n",
        "X_train, X_val, y_train, y_val = hm.split_data(X, y, 0.2, rng)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8AEbmm_lz1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Thanks to https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n",
        "# Load tokenizer from the bert layer\n",
        "# We will use the official tokenization script created by the Google team\n",
        "import tokenization\n",
        "\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcUOTOAEl1W5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode the text into tokens, masks, and segment flags\n",
        "\n",
        "train_input = bert_encode(X_train, tokenizer, max_len=160)\n",
        "train_labels = y_train\n",
        "\n",
        "val_input = bert_encode(X_val, tokenizer, max_len=160)\n",
        "val_labels = y_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGPR1k4bve7e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfoq3tSucYVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {\n",
        "    'model_id': 'model_BERT.h5',\n",
        "    'max_len': 160,\n",
        "    'epochs': 5,\n",
        "    'batch_size': 16\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGz-mEn_mNDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build BERT model with my tuning\n",
        "model_BERT = build_model(bert_layer, params['max_len'])\n",
        "model_BERT.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qw-asu73oZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint(params['model_id'] , monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "train_history = model_BERT.fit(\n",
        "    train_input, train_labels,\n",
        "    validation_data=(val_input, val_labels),\n",
        "    epochs = params['epochs'], # recomended 3-5 epochs\n",
        "    callbacks=[checkpoint],\n",
        "    batch_size = params['batch_size']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvo3PS4fI6qr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission_bert(params['model_id'], tokenizer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq27k4amLja6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(re.sub('.h5', '',params['model_id']) + '_submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}